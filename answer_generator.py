from retriever import retriever
def funcanswergenerator():
    query, results=retriever()
    #STEP1:Extract Retrieved Context
    retrieved_chunks=[]

    if not results:
        raise ValueError("No documents retrieved for the query")

    for idx, doc in enumerate(results):
        retrieved_chunks.append({
            "chunk_id":f"C{idx+1}",
            "text":doc.page_content.strip()
        })
    
    #STEP2:Build Prompt
    def build_prompt(query, chunks):
        context_block=""
        for chunk in chunks:
            context_block+=f"[{chunk['chunk_id']}]\n{chunk['text']}\n\n"
        
        prompt=f"""
You are an assistant answering a question using ONLY the provided text snippets.

Question:
{query}

Retrieved Text Chunks:
{context_block}

Instructions:
1. Answer the question using ONLY the retrieved text.
2. Do NOT introduce external knowledge.
3. Cite chunk IDs (e.g., C1, C2) for every factual claim.
4. If the answer is not present, say "The retrieved text does not contain sufficient information."
5. Output STRICT JSON only. Do NOT include explanations or extra text.

Output Format (STRICT JSON):
{{
  "answer": "...",
  "supporting_chunks": ["C1", "C2"]
}}
"""
        return prompt
    
    #STEP3:Call LLaMA-3 via Ollama
    import requests
    import json

    OLLAMA_URL="http://localhost:11434/api/generate"

    prompt = build_prompt(query, retrieved_chunks)

    payload={
        "model":"llama3:8b",
        "prompt":prompt,
        "stream":False,
        "options":{
            "temperature":0.0
        }
    }

    response=requests.post(OLLAMA_URL,json=payload, timeout=120)

    if response.status_code!=200:
        raise RuntimeError("Ollama request failed")
    
    response_json=response.json()
    if "response" not in response_json:
        raise RuntimeError("Ollama response missing 'response' field")
    raw_output=response_json["response"]

    #STEP4:Parse & Validate Output
    try:
        parsed_output=json.loads(raw_output)
    except json.JSONDecodeError:
        raise ValueError("LLM output is not a valid JSON")
    
    answer=parsed_output.get("answer", "No answer generated.").strip()
    supporting_chunks=parsed_output.get("supporting_chunks",[])

    if not answer:
        raise ValueError("Empty answer generated by LLM")

    if not isinstance(supporting_chunks, list):
        raise ValueError("supporting_chunks must be a list")
    
    valid_ids={c["chunk_id"] for c in retrieved_chunks}

    if answer and not any(cid in valid_ids for cid in supporting_chunks):
        raise ValueError("Answer generated without valid supporting evidence")

    print("\nFinal Answer : \n")
    print(answer)
    return answer